## Word2Vec
# Friends-Word2Vec

Implemented Word2Vec on Friends Dataset, 
Word2Vec was trained using Google Colab on Intel Xeon CPU. The dataset used for training is an open-source dataset available at Kaggle.

A corpus size of 115858 was used to train the Word2Vec model, which helped build a vocabulary of size 10550. Each word is represented by a 100-dimensional feature vector.
Using PCA, feature vector was reduced to 3D which was further used in Visualization of Feature vectors in 3D space.


**List of requirements**

* [Python](https://www.python.org/downloads/) (3.11.1)
* [Numpy](https://github.com/numpy/numpy) (1.21.6)
* [Pandas](https://github.com/pandas-dev/pandas) (1.3.5)
* [Gensim](https://github.com/RaRe-Technologies/gensim) (3.6.0)
* [Nltk](https://github.com/nltk/nltk) (3.7)
* [scikit-learn ](https://github.com/scikit-learn/scikit-learn) (1.2.1)
* [Ploty Express](https://plotly.com/python/plotly-express/) (5.13.0)
